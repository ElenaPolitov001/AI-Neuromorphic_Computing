{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX-Runtime InferenceSession Object 분석\n",
    "- ONNX Runtime 의 InferenceSession Obejct 분석\n",
    "- 출처 : https://github.com/microsoft/onnxruntime#inferencing-start // session.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출처 : https://github.com/microsoft/onnxruntime#inferencing-start // session.py\n",
    "#-------------------------------------------------------------------------\n",
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License.\n",
    "#--------------------------------------------------------------------------\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from onnxruntime.capi import _pybind_state as C\n",
    "\n",
    "# 추론 Session\n",
    "class InferenceSession:\n",
    "    \n",
    "    # 생성자 - (파일 이름, sess_options)\n",
    "    def __init__(self, path_or_bytes, sess_options=None):\n",
    "        \"\"\"\n",
    "        :param path_or_bytes: filename or serialized model in a byte string\n",
    "        :param sess_options: session options\n",
    "        \"\"\"\n",
    "        self._path_or_bytes = path_or_bytes # file 이름\n",
    "        self._sess_options = sess_options # default none. session option\n",
    "        self._load_model() # onnx 모델 읽어옴. \n",
    "        self._enable_fallback = True \n",
    "\n",
    "    # onnx 모델 읽어오는 것.\n",
    "    def _load_model(self, providers=[]):\n",
    "        if self._sess_options:\n",
    "            self._sess = C.InferenceSession(\n",
    "                self._sess_options, C.get_session_initializer())\n",
    "        else:\n",
    "            self._sess = C.InferenceSession(\n",
    "                C.get_session_initializer(), C.get_session_initializer())\n",
    "\n",
    "        if isinstance(self._path_or_bytes, str):\n",
    "            self._sess.load_model(self._path_or_bytes, providers)\n",
    "        elif isinstance(self._path_or_bytes, bytes):\n",
    "            self._sess.read_bytes(self._path_or_bytes, providers)\n",
    "        elif isinstance(self._path_or_bytes, tuple):\n",
    "            # to remove, hidden trick\n",
    "            self._sess.load_model_no_init(self._path_or_bytes[0], providers)\n",
    "        else:\n",
    "            raise TypeError(\"Unable to load from type '{0}'\".format(type(self._path_or_bytes)))\n",
    "\n",
    "        self._inputs_meta = self._sess.inputs_meta\n",
    "        self._outputs_meta = self._sess.outputs_meta\n",
    "        self._overridable_initializers = self._sess.overridable_initializers\n",
    "        self._model_meta = self._sess.model_meta\n",
    "        self._providers = self._sess.get_providers()\n",
    "\n",
    "        # Tensorrt can fall back to CUDA. All others fall back to CPU.\n",
    "        if 'TensorrtExecutionProvider' in C.get_available_providers():\n",
    "          self._fallback_providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    "        else:\n",
    "          self._fallback_providers = ['CPUExecutionProvider']\n",
    "\n",
    "    def _reset_session(self):\n",
    "        \"release underlying session object.\"\n",
    "        # meta data references session internal structures\n",
    "        # so they must be set to None to decrement _sess reference count.\n",
    "        self._inputs_meta = None\n",
    "        self._outputs_meta = None\n",
    "        self._overridable_initializers = None\n",
    "        self._model_meta = None\n",
    "        self._providers = None\n",
    "        self._sess = None\n",
    "\n",
    "    def get_inputs(self):\n",
    "        \"Return the inputs metadata as a list of :class:`onnxruntime.NodeArg`.\"\n",
    "        return self._inputs_meta\n",
    "\n",
    "    def get_outputs(self):\n",
    "        \"Return the outputs metadata as a list of :class:`onnxruntime.NodeArg`.\"\n",
    "        return self._outputs_meta\n",
    "\n",
    "    def get_overridable_initializers(self):\n",
    "        \"Return the inputs (including initializers) metadata as a list of :class:`onnxruntime.NodeArg`.\"\n",
    "        return self._overridable_initializers\n",
    "\n",
    "    def get_modelmeta(self):\n",
    "        \"Return the metadata. See :class:`onnxruntime.ModelMetadata`.\"\n",
    "        return self._model_meta\n",
    "\n",
    "    def get_providers(self):\n",
    "        \"Return list of registered execution providers.\"\n",
    "        return self._providers\n",
    "\n",
    "    def set_providers(self, providers):\n",
    "        \"\"\"\n",
    "        Register the input list of execution providers. The underlying session is re-created.\n",
    "\n",
    "        :param providers: list of execution providers\n",
    "\n",
    "        The list of providers is ordered by Priority. For example ['CUDAExecutionProvider', 'CPUExecutionProvider'] means\n",
    "        execute a node using CUDAExecutionProvider if capable, otherwise execute using CPUExecutionProvider.\n",
    "        \"\"\"\n",
    "        if not set(providers).issubset(C.get_available_providers()):\n",
    "            raise ValueError(\"{} does not contain a subset of available providers {}\".format(providers, C.get_available_providers()))\n",
    "        self._reset_session()\n",
    "        self._load_model(providers)\n",
    "\n",
    "    def disable_fallback(self):\n",
    "        \"\"\"\n",
    "        Disable session.run() fallback mechanism.\n",
    "        \"\"\"\n",
    "        self._enable_fallback = False\n",
    "\n",
    "    def enable_fallback(self):\n",
    "        \"\"\"\n",
    "        Enable session.Run() fallback mechanism. If session.Run() fails due to an internal Execution Provider failure, reset the Execution Providers\n",
    "        enabled for this session.\n",
    "        If GPU is enabled, fall back to CUDAExecutionProvider.\n",
    "        otherwise fall back to CPUExecutionProvider.\n",
    "        \"\"\"\n",
    "        self._enable_fallback = True\n",
    "\n",
    "    def run(self, output_names, input_feed, run_options=None):\n",
    "        \"\"\"\n",
    "        Compute the predictions.\n",
    "\n",
    "        :param output_names: name of the outputs\n",
    "        :param input_feed: dictionary ``{ input_name: input_value }``\n",
    "        :param run_options: See :class:`onnxruntime.RunOptions`.\n",
    "\n",
    "        ::\n",
    "\n",
    "            sess.run([output_name], {input_name: x})\n",
    "        \"\"\"\n",
    "        num_required_inputs = len(self._inputs_meta)\n",
    "        num_inputs = len(input_feed)\n",
    "        # the graph may have optional inputs used to override initializers. allow for that.\n",
    "        if num_inputs < num_required_inputs:\n",
    "            raise ValueError(\"Model requires {} inputs. Input Feed contains {}\".format(num_required_inputs, num_inputs))\n",
    "        if not output_names:\n",
    "            output_names = [output.name for output in self._outputs_meta]\n",
    "        try:\n",
    "            return self._sess.run(output_names, input_feed, run_options)\n",
    "        except C.EPFail as err:\n",
    "            if self._enable_fallback:\n",
    "                print(\"EP Error: {} using {}\".format(str(err), self._providers))\n",
    "                print(\"Falling back to {} and retrying.\".format(self._fallback_providers))\n",
    "                self.set_providers(self._fallback_providers)\n",
    "                # Fallback only once.\n",
    "                self.disable_fallback()\n",
    "                return self._sess.run(output_names, input_feed, run_options)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "    def end_profiling(self):\n",
    "        \"\"\"\n",
    "        End profiling and return results in a file.\n",
    "\n",
    "        The results are stored in a filename if the option\n",
    "        :meth:`onnxruntime.SessionOptions.enable_profiling`.\n",
    "        \"\"\"\n",
    "        return self._sess.end_profiling()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
