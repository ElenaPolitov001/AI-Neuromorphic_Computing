{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare DNN Model\n",
    "- Compare Keras vs onnx runtime\n",
    "\n",
    "## Make DNN Model and Convert ONNX\n",
    "- Keras로 DNN 모델 생성 \n",
    "- 학습은 Mnist Data로 진행\n",
    "- <b> .h5 모델 로드 후 .onnx로 변환 </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (784,)\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "48000/48000 [==============================] - 2s 32us/sample - loss: 1.0111 - accuracy: 0.7331 - val_loss: 0.3708 - val_accuracy: 0.8951\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import tensorflow\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import os\n",
    "os.environ['TF_KERAS'] = '1'\n",
    "\n",
    "# Configuration options\n",
    "feature_vector_length = 784\n",
    "num_classes = 10\n",
    "\n",
    "# Load the data\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "\n",
    "# Reshape the data - MLPs do not understand such things as '2D'.\n",
    "# Reshape to 28 x 28 pixels = 784 features\n",
    "X_train = X_train.reshape(X_train.shape[0], feature_vector_length)\n",
    "X_test = X_test.reshape(X_test.shape[0], feature_vector_length)\n",
    "\n",
    "# Convert into greyscale\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Convert target classes to categorical ones\n",
    "Y_train = to_categorical(Y_train, num_classes)\n",
    "Y_test = to_categorical(Y_test, num_classes)\n",
    "\n",
    "# Set the input shape\n",
    "input_shape = (feature_vector_length,)\n",
    "print(f'Feature shape: {input_shape}')\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Dense(350, input_shape=input_shape, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Configure the model and start training\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, epochs=1, batch_size=2000, verbose=1, validation_split=0.2)\n",
    "\n",
    "model.save('model/DNN_MLP_basic.h5')\n",
    "\n",
    "# Test the model after training\n",
    "# test_results = model.evaluate(X_test, Y_test, verbose=1)\n",
    "# print(f'Test results - Loss: {test_results[0]} - Accuracy: {test_results[1]}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The maximum opset needed by this model is only 9.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx로 변환 완료\n"
     ]
    }
   ],
   "source": [
    "# Convert into ONNX Format\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras2onnx import convert_keras\n",
    "import onnxruntime as rt\n",
    "import numpy as np\n",
    "\n",
    "model = load_model('model/DNN_MLP_basic.h5')\n",
    "\n",
    "# keras model -> onnx 로 변환\n",
    "onx = convert_keras(model, 'model/DNN_MLP_basic.onnx')\n",
    "\n",
    "with open(\"model/DNN_MLP_basic.onnx\", \"wb\") as f:\n",
    "    f.write(onx.SerializeToString())\n",
    "\n",
    "print('onnx로 변환 완료')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_test 10장만 테스트\n",
    "X_test = X_test[0:10]\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keras - DNN 걸린시간: 0.3484160900115967\n"
     ]
    }
   ],
   "source": [
    "# lenet - mnist\n",
    "import time\n",
    "\n",
    "# 시간측정\n",
    "start = time.time()\n",
    "# lenet - keras \n",
    "for i in range(10):\n",
    "    pred_keras = model.predict(X_test)\n",
    "end = time.time()\n",
    "print('keras - DNN 걸린시간:', np.double(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnxruntime DNN 걸린시간: 0.003989696502685547\n"
     ]
    }
   ],
   "source": [
    "# Compute the prediction with ONNX Runtime\n",
    "sess = rt.InferenceSession('model/DNN_MLP_basic.onnx')\n",
    "input_name = sess.get_inputs()[0].name\n",
    "label_name = sess.get_outputs()[0].name\n",
    "\n",
    "# 시간측정\n",
    "start = time.time()\n",
    "\n",
    "# lenet - onnxruntime \n",
    "for i in range(10):\n",
    "    pred_onx = sess.run([label_name], {input_name: X_test.astype(np.float32)})[0]\n",
    "end = time.time()\n",
    "\n",
    "print('onnxruntime DNN 걸린시간:', np.double(end-start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
